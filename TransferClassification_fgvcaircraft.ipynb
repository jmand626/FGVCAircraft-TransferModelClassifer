{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmand626/FGVCAircraft-TransferModelClassifer/blob/main/TransferClassification_fgvcaircraft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the first cell, where image_path is defined\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the path to the data folder\n",
        "data_path = Path(\"/content/drive/MyDrive/data/\")\n",
        "image_path = data_path / \"fgvc_aircraft\""
      ],
      "metadata": {
        "id": "9Sk080gYlXua"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "# 1️ Mount Google Drive (if using for storage)\n",
        "use_gdrive = True  # Set to True if dataset is stored in Google Drive\n",
        "if use_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 2️ Clone your GitHub repo if it's not already present\n",
        "repo_url = \"https://github.com/jmand626/PyTorchMLEngine-Custom-Dataset-Project.git\"\n",
        "repo_name = \"PyTorchMLEngine-Custom-Dataset-Project\"\n",
        "\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"Cloning {repo_url}...\")\n",
        "    !git clone {repo_url}\n",
        "else:\n",
        "    print(f\"Repository {repo_name} already exists.\")\n",
        "\n",
        "# 3️ Change to repo directory ONLY ONCE\n",
        "os.chdir(repo_name) # This line sets the working directory\n",
        "\n",
        "# 4️ Add project files to sys.path so imports work\n",
        "sys.path.append(os.getcwd())\n",
        "print(\"Project directory added to sys.path\")\n",
        "\n",
        "# 5️ Ensure necessary dependencies are installed\n",
        "try:\n",
        "    import torchinfo\n",
        "except ImportError:\n",
        "    print(\"Installing torchinfo...\")\n",
        "    !pip install -q torchinfo\n",
        "\n",
        "# 6️ Download FGVC Aircraft dataset if missing\n",
        "dataset_url = \"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/fgvc-aircraft-2013b.tar.gz\"\n",
        "dataset_tar = data_path / \"fgvc-aircraft-2013b.tar.gz\"\n",
        "dataset_folder = data_path / \"fgvc-aircraft-2013b\"\n",
        "# Define a file within the extracted dataset to check for existence\n",
        "check_file = dataset_folder / \"data/images/0034309.jpg\"\n",
        "\n",
        "if check_file.exists():\n",
        "    print(\"Dataset already exists.\")\n",
        "else:\n",
        "    print(\"Downloading FGVC Aircraft dataset...\")\n",
        "    data_path.mkdir(parents=True, exist_ok=True)\n",
        "    response = requests.get(dataset_url, stream=True)\n",
        "    with open(dataset_tar, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "    print(\"Extracting dataset...\")\n",
        "    !tar -xzf {dataset_tar} -C {data_path}\n",
        "    os.remove(dataset_tar)\n",
        "    print(\"Dataset extraction complete.\")"
      ],
      "metadata": {
        "id": "GzSakdlhzbLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6591ad-2d9d-4e22-d950-08967a03ec95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning https://github.com/jmand626/PyTorchMLEngine-Custom-Dataset-Project.git...\n",
            "Cloning into 'PyTorchMLEngine-Custom-Dataset-Project'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 24 (delta 6), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (24/24), 21.93 KiB | 21.93 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "Project directory added to sys.path\n",
            "Installing torchinfo...\n",
            "Dataset already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xPSxUj-_Teyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c07881c8-68a9-4ab7-dc74-811592c606ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "torch version: 2.8.0+cu126\n",
            "torchvision version: 0.23.0+cu126\n"
          ]
        }
      ],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "OpCWZh7Phbna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc3d100-a8b4-4076-bf28-8982919ca247"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computer_vision_test_main.py  firsttry_model.py  README.md\n",
            "create_custom_dataset.py      model_backbone.py  setup_dataholders.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "jt9KA4KSiTYJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dd4b5ad7-5362-4881-b519-858c782abe76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now hopefully we can continously use the previous setup code whenever we want to use this dataset again."
      ],
      "metadata": {
        "id": "cMp0N0R2pW_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/data/fgvc-aircraft-2013b/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSEoF7y_p6ig",
        "outputId": "b2ae383d-d068-4fc3-89c0-2942cb0bfe18"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "families.txt\t\t\t  images_manufacturer_val.txt\n",
            "images\t\t\t\t  images_test.txt\n",
            "images_box.txt\t\t\t  images_train.txt\n",
            "images_family_test.txt\t\t  images_val.txt\n",
            "images_family_train.txt\t\t  images_variant_test.txt\n",
            "images_family_trainval.txt\t  images_variant_train.txt\n",
            "images_family_val.txt\t\t  images_variant_trainval.txt\n",
            "images_manufacturer_test.txt\t  images_variant_val.txt\n",
            "images_manufacturer_train.txt\t  manufacturers.txt\n",
            "images_manufacturer_trainval.txt  variants.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-12-e84a44c78b2d\n",
        "# Assume the dataset is extracted to 'data/fgvc-aircraft-2013b'\n",
        "# and images are in 'data/fgvc-aircraft-2013b/data/images'\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Fix: Update paths to include the subfolder where the dataset was downloaded and extracted\n",
        "# Use os.path.join to create platform-independent paths\n",
        "# The issue was train_dir and test_dir were pointing to the wrong location.\n",
        "# They should point to the parent directory containing the class folders.\n",
        "# After reorganizing, the class folders will be inside the 'images' directory.\n",
        "\n",
        "# Corrected paths to point to the dataset location on Google Drive\n",
        "# Based on the dataset download path in cell GzSakdlhzbLN\n",
        "data_path = Path(\"/content/drive/MyDrive/data/\") # This variable is defined in the first cell\n",
        "dataset_folder = data_path / \"fgvc-aircraft-2013b\"\n",
        "\n",
        "train_dir = dataset_folder / \"data\" / \"images\" # Corrected path\n",
        "test_dir = dataset_folder / \"data\" / \"images\"  # Corrected path, assuming test images are in the same location\n",
        "\n",
        "\n",
        "# Print the resolved paths to verify they are correct\n",
        "print(\"Train directory:\", train_dir)\n",
        "print(\"Test directory:\", test_dir)"
      ],
      "metadata": {
        "id": "tKnebh8NpT7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa83a51c-8cd8-412b-c8b2-4a0285ccbb63"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train directory: /content/drive/MyDrive/data/fgvc-aircraft-2013b/data/images\n",
            "Test directory: /content/drive/MyDrive/data/fgvc-aircraft-2013b/data/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we continue on to creating our datasets and dataloaders. An important issue is that we have to ensure that the data that we feed into our pretrained model must be formatted in the same way as the data inputted when training the model (as that helps performance immeasurably). There is a certain way that all models from torchvision.models require, and we will do that.\n",
        "\n",
        "It is detailed in this page: https://docs.pytorch.org/vision/0.8/models.html"
      ],
      "metadata": {
        "id": "JWvJSUvVtJYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import importlib\n",
        "import setup_dataholders\n",
        "importlib.reload(setup_dataholders)\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each color channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each color channel),\n",
        "])"
      ],
      "metadata": {
        "id": "I2wsd6E2t64w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Define the path to the data folder on Google Drive\n",
        "data_path = Path(\"/content/drive/MyDrive/data/\")\n",
        "dataset_folder = data_path / \"fgvc-aircraft-2013b\"\n",
        "images_base_dir = dataset_folder / \"data\" / \"images\" # Base directory where images are now in class subfolders\n",
        "\n",
        "# Define paths to the train and test mapping files\n",
        "train_mapping_file = dataset_folder / \"data\" / \"images_variant_train.txt\"\n",
        "test_mapping_file = dataset_folder / \"data\" / \"images_variant_test.txt\"\n",
        "variants_file = dataset_folder / \"data\" / \"variants.txt\" # File containing all class names\n",
        "\n",
        "# Define the target root directories for the train and test splits\n",
        "train_root_dir = dataset_folder / \"train\"\n",
        "test_root_dir = dataset_folder / \"test\"\n",
        "\n",
        "# Function to check if the directories are populated\n",
        "def is_populated(directory):\n",
        "    if not directory.exists():\n",
        "        return False\n",
        "    # Check if there are any subdirectories (which represent classes)\n",
        "    if not any(directory.iterdir()):\n",
        "        return False\n",
        "    # You could add a more robust check here, like checking if a certain number of\n",
        "    # class directories exist or if a specific file exists in a class directory.\n",
        "    return True\n",
        "\n",
        "# Check if directories are already populated\n",
        "if is_populated(train_root_dir) and is_populated(test_root_dir):\n",
        "    print(\"Train and test directories are already populated. Skipping population.\")\n",
        "else:\n",
        "    print(\"Train and test directories are not populated or incomplete. Populating now...\")\n",
        "\n",
        "    # Ensure the target root directories exist and are empty or can be overwritten if not populated\n",
        "    for dir_path in [train_root_dir, test_root_dir]:\n",
        "        if dir_path.exists() and not is_populated(dir_path):\n",
        "            print(f\"Removing incomplete existing directory: {dir_path}\")\n",
        "            shutil.rmtree(dir_path)\n",
        "        dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Get the full list of class names from variants.txt\n",
        "    all_class_names = []\n",
        "    with open(variants_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            all_class_names.append(line.strip())\n",
        "    all_class_names = sorted(list(set(all_class_names))) # Remove duplicates and sort\n",
        "\n",
        "    print(f\"Found {len(all_class_names)} unique class names from variants.txt.\")\n",
        "\n",
        "    # Create all class subdirectories within the train and test root directories\n",
        "    print(\"Creating all class subdirectories in train and test folders...\")\n",
        "    for class_name in all_class_names:\n",
        "        sanitized_class_folder_name = class_name.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
        "        (train_root_dir / sanitized_class_folder_name).mkdir(parents=True, exist_ok=True)\n",
        "        (test_root_dir / sanitized_class_folder_name).mkdir(parents=True, exist_ok=True)\n",
        "    print(\"Class subdirectories created.\")\n",
        "\n",
        "\n",
        "    # Function to read mapping files and get image ID to label mapping\n",
        "    def read_mapping_file(filepath):\n",
        "        image_id_to_class = {}\n",
        "        with open(filepath, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(\" \", 1)\n",
        "                if len(parts) == 2:\n",
        "                    image_id, label = parts\n",
        "                    image_id_to_class[image_id] = label\n",
        "                else:\n",
        "                     print(f\"Warning: Skipping line in {filepath.name} with unexpected format: {line.strip()}\")\n",
        "        return image_id_to_class\n",
        "\n",
        "    # Read mappings for train and test sets\n",
        "    train_image_id_to_class = read_mapping_file(train_mapping_file)\n",
        "    test_image_id_to_class = read_mapping_file(test_mapping_file)\n",
        "\n",
        "    print(f\"Found mappings for {len(train_image_id_to_class)} training images.\")\n",
        "    print(f\"Found mappings for {len(test_image_id_to_class)} testing images.\")\n",
        "\n",
        "    # Function to copy images for a given split based on mappings\n",
        "    def populate_split_directories(image_id_to_class_mapping, target_root_dir, images_base_dir):\n",
        "        copied_count = 0\n",
        "        source_not_found_count = 0\n",
        "        target_folder_not_found_count = 0\n",
        "\n",
        "        print(f\"Populating {target_root_dir} with images...\")\n",
        "\n",
        "        for image_id, class_label in image_id_to_class_mapping.items():\n",
        "            image_name = f\"{image_id}.jpg\"\n",
        "            sanitized_class_folder_name = class_label.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
        "\n",
        "            # Source path: Look for the image within the existing class subfolders in images_base_dir\n",
        "            source_image_path = images_base_dir / sanitized_class_folder_name / image_name\n",
        "\n",
        "            # Target path: The location in the new train/test class folder (already created)\n",
        "            target_class_folder = target_root_dir / sanitized_class_folder_name\n",
        "            target_image_path = target_class_folder / image_name\n",
        "\n",
        "            if source_image_path.exists():\n",
        "                if target_class_folder.exists(): # Ensure target class folder was created\n",
        "                    if not target_image_path.exists():\n",
        "                        try:\n",
        "                            shutil.copy(str(source_image_path), str(target_image_path))\n",
        "                            copied_count += 1\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error copying file {source_image_path} to {target_class_folder}: {e}\")\n",
        "                else:\n",
        "                    print(f\"Error: Target class folder not found for {class_label}: {target_class_folder}\")\n",
        "                    target_folder_not_found_count += 1\n",
        "            else:\n",
        "                print(f\"Warning: Source image file not found: {source_image_path}\")\n",
        "                source_not_found_count += 1\n",
        "\n",
        "        print(f\"Finished populating {target_root_dir}. Copied {copied_count} images. {source_not_found_count} source images not found. {target_folder_not_found_count} target class folders not found.\")\n",
        "\n",
        "\n",
        "    # Populate train and test directories\n",
        "    populate_split_directories(train_image_id_to_class, train_root_dir, images_base_dir)\n",
        "    populate_split_directories(test_image_id_to_class, test_root_dir, images_base_dir)\n",
        "\n",
        "\n",
        "    print(\"Dataset reorganization and population complete.\")"
      ],
      "metadata": {
        "id": "WjkCCglEFW_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b896e5-1087-4fc0-94be-b431e9a8754b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and test directories are already populated. Skipping population.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/drive/MyDrive/data/fgvc-aircraft-2013b/train\"\n",
        "test_dir = \"/content/drive/MyDrive/data/fgvc-aircraft-2013b/test\""
      ],
      "metadata": {
        "id": "qOfADclIuNNQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = setup_dataholders.create_dataloaders(train_directory=train_dir,\n",
        "                                                                               test_directory=test_dir,\n",
        "                                                                               data_transforms=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n",
        "                                                                               batch_size=32, # set mini-batch size to 32\n",
        "                                                                               workers=4)\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "TC7PU_cxr7He",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2850f91c-2b52-4935-ba0b-70df5bd021c0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7c6b30856930>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7c6b25ec22a0>,\n",
              " ['707_320',\n",
              "  '727_200',\n",
              "  '737_200',\n",
              "  '737_300',\n",
              "  '737_400',\n",
              "  '737_500',\n",
              "  '737_600',\n",
              "  '737_700',\n",
              "  '737_800',\n",
              "  '737_900',\n",
              "  '747_100',\n",
              "  '747_200',\n",
              "  '747_300',\n",
              "  '747_400',\n",
              "  '757_200',\n",
              "  '757_300',\n",
              "  '767_200',\n",
              "  '767_300',\n",
              "  '767_400',\n",
              "  '777_200',\n",
              "  '777_300',\n",
              "  'A300B4',\n",
              "  'A310',\n",
              "  'A318',\n",
              "  'A319',\n",
              "  'A320',\n",
              "  'A321',\n",
              "  'A330_200',\n",
              "  'A330_300',\n",
              "  'A340_200',\n",
              "  'A340_300',\n",
              "  'A340_500',\n",
              "  'A340_600',\n",
              "  'A380',\n",
              "  'ATR_42',\n",
              "  'ATR_72',\n",
              "  'An_12',\n",
              "  'BAE_125',\n",
              "  'BAE_146_200',\n",
              "  'BAE_146_300',\n",
              "  'Beechcraft_1900',\n",
              "  'Boeing_717',\n",
              "  'CRJ_200',\n",
              "  'CRJ_700',\n",
              "  'CRJ_900',\n",
              "  'C_130',\n",
              "  'C_47',\n",
              "  'Cessna_172',\n",
              "  'Cessna_208',\n",
              "  'Cessna_525',\n",
              "  'Cessna_560',\n",
              "  'Challenger_600',\n",
              "  'DC_10',\n",
              "  'DC_3',\n",
              "  'DC_6',\n",
              "  'DC_8',\n",
              "  'DC_9_30',\n",
              "  'DHC_1',\n",
              "  'DHC_6',\n",
              "  'DHC_8_100',\n",
              "  'DHC_8_300',\n",
              "  'DH_82',\n",
              "  'DR_400',\n",
              "  'Dornier_328',\n",
              "  'EMB_120',\n",
              "  'ERJ_135',\n",
              "  'ERJ_145',\n",
              "  'E_170',\n",
              "  'E_190',\n",
              "  'E_195',\n",
              "  'Embraer_Legacy_600',\n",
              "  'Eurofighter_Typhoon',\n",
              "  'F_16A_B',\n",
              "  'F_A_18',\n",
              "  'Falcon_2000',\n",
              "  'Falcon_900',\n",
              "  'Fokker_100',\n",
              "  'Fokker_50',\n",
              "  'Fokker_70',\n",
              "  'Global_Express',\n",
              "  'Gulfstream_IV',\n",
              "  'Gulfstream_V',\n",
              "  'Hawk_T1',\n",
              "  'Il_76',\n",
              "  'L_1011',\n",
              "  'MD_11',\n",
              "  'MD_80',\n",
              "  'MD_87',\n",
              "  'MD_90',\n",
              "  'Metroliner',\n",
              "  'Model_B200',\n",
              "  'PA_28',\n",
              "  'SR_20',\n",
              "  'Saab_2000',\n",
              "  'Saab_340',\n",
              "  'Spitfire',\n",
              "  'Tornado',\n",
              "  'Tu_134',\n",
              "  'Tu_154',\n",
              "  'Yak_42'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cells focus on the actual \"transfer\" part of taking a model from someplace else and using it for better performance. I could have done the transforms from the previous cells in a different way that is more automatic, but I wished to explore the more manual original way first."
      ],
      "metadata": {
        "id": "xG8FesLvNe3c"
      }
    }
  ]
}