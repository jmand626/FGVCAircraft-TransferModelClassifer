{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ac90e26fa9f47009549e22f73170029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70ec9ec658ec4125964f02351ffd5976",
              "IPY_MODEL_29de0981d2f341b581573b7250035a57",
              "IPY_MODEL_414d7c2038e3468b8a398e40b4e8aeac"
            ],
            "layout": "IPY_MODEL_a1b8f9d921044ef1aef754021121141e"
          }
        },
        "70ec9ec658ec4125964f02351ffd5976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b05200401df4109b3606d689f5ae176",
            "placeholder": "​",
            "style": "IPY_MODEL_f907664db6a74661b4d84ef87c226693",
            "value": "Training Epochs:   0%"
          }
        },
        "29de0981d2f341b581573b7250035a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_711cfba8dbe14c1da398704d41387765",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14bd0499e0774801ab0905ce651e13e1",
            "value": 0
          }
        },
        "414d7c2038e3468b8a398e40b4e8aeac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c167c71658184efb9943b0a3d40880d9",
            "placeholder": "​",
            "style": "IPY_MODEL_b252ab4183e44b6db7ca8e5c70c1bd8e",
            "value": " 0/5 [04:59&lt;?, ?it/s]"
          }
        },
        "a1b8f9d921044ef1aef754021121141e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b05200401df4109b3606d689f5ae176": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f907664db6a74661b4d84ef87c226693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "711cfba8dbe14c1da398704d41387765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14bd0499e0774801ab0905ce651e13e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c167c71658184efb9943b0a3d40880d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b252ab4183e44b6db7ca8e5c70c1bd8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmand626/FGVCAircraft-TransferModelClassifer/blob/main/TransferClassification_fgvcaircraft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the first cell, where image_path is defined\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the path to the data folder\n",
        "data_path = Path(\"/content/drive/MyDrive/data/\")\n",
        "image_path = data_path / \"fgvc_aircraft\""
      ],
      "metadata": {
        "id": "9Sk080gYlXua"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "# 1️ Mount Google Drive (if using for storage)\n",
        "use_gdrive = True  # Set to True if dataset is stored in Google Drive\n",
        "if use_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 2️ Clone your GitHub repo if it's not already present\n",
        "repo_url = \"https://github.com/jmand626/PyTorchMLEngine-Custom-Dataset-Project.git\"\n",
        "repo_name = \"PyTorchMLEngine-Custom-Dataset-Project\"\n",
        "\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"Cloning {repo_url}...\")\n",
        "    !git clone {repo_url}\n",
        "else:\n",
        "    print(f\"Repository {repo_name} already exists.\")\n",
        "\n",
        "# 3️ Change to repo directory ONLY ONCE\n",
        "os.chdir(repo_name) # This line sets the working directory\n",
        "\n",
        "# 4️ Add project files to sys.path so imports work\n",
        "sys.path.append(os.getcwd())\n",
        "print(\"Project directory added to sys.path\")\n",
        "\n",
        "# 5️ Ensure necessary dependencies are installed\n",
        "try:\n",
        "    import torchinfo\n",
        "except ImportError:\n",
        "    print(\"Installing torchinfo...\")\n",
        "    !pip install -q torchinfo\n",
        "\n",
        "# 6️ Download FGVC Aircraft dataset if missing\n",
        "dataset_url = \"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/fgvc-aircraft-2013b.tar.gz\"\n",
        "dataset_tar = data_path / \"fgvc-aircraft-2013b.tar.gz\"\n",
        "dataset_folder = data_path / \"fgvc-aircraft-2013b\"\n",
        "# Define a file within the extracted dataset to check for existence\n",
        "check_file = dataset_folder / \"data/images/0034309.jpg\"\n",
        "\n",
        "if check_file.exists():\n",
        "    print(\"Dataset already exists.\")\n",
        "else:\n",
        "    print(\"Downloading FGVC Aircraft dataset...\")\n",
        "    data_path.mkdir(parents=True, exist_ok=True)\n",
        "    response = requests.get(dataset_url, stream=True)\n",
        "    with open(dataset_tar, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "    print(\"Extracting dataset...\")\n",
        "    !tar -xzf {dataset_tar} -C {data_path}\n",
        "    os.remove(dataset_tar)\n",
        "    print(\"Dataset extraction complete.\")"
      ],
      "metadata": {
        "id": "GzSakdlhzbLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f1ef82-c33d-4045-d306-5a4b7029e0de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Repository PyTorchMLEngine-Custom-Dataset-Project already exists.\n",
            "Project directory added to sys.path\n",
            "Dataset already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xPSxUj-_Teyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c10d7f-2d8c-4f1c-866b-3006b7b8aa43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "torch version: 2.9.0+cu128\n",
            "torchvision version: 0.24.0+cu128\n"
          ]
        }
      ],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "OpCWZh7Phbna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38446d09-39ff-4d18-be61-bac93cf2272c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computer_vision_test_main.py  model_backbone.py  setup_dataholders.py\n",
            "create_custom_dataset.py      __pycache__\n",
            "firsttry_model.py\t      README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import model_backbone\n",
        "import setup_dataholders"
      ],
      "metadata": {
        "id": "igLk5J4rC7EI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "jt9KA4KSiTYJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d15e0a6d-1c73-4c1a-b81d-6484f40375a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now hopefully we can continously use the previous setup code whenever we want to use this dataset again."
      ],
      "metadata": {
        "id": "cMp0N0R2pW_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/data/fgvc-aircraft-2013b/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSEoF7y_p6ig",
        "outputId": "9f425b31-b082-4a72-ac41-0969af0d7186"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "families.txt\t\t\t  images_manufacturer_val.txt\n",
            "images\t\t\t\t  images_test.txt\n",
            "images_box.txt\t\t\t  images_train.txt\n",
            "images_family_test.txt\t\t  images_val.txt\n",
            "images_family_train.txt\t\t  images_variant_test.txt\n",
            "images_family_trainval.txt\t  images_variant_train.txt\n",
            "images_family_val.txt\t\t  images_variant_trainval.txt\n",
            "images_manufacturer_test.txt\t  images_variant_val.txt\n",
            "images_manufacturer_train.txt\t  manufacturers.txt\n",
            "images_manufacturer_trainval.txt  variants.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Define the path to the data folder on Google Drive\n",
        "data_path = Path(\"/content/drive/MyDrive/data/\")\n",
        "dataset_folder = data_path / \"fgvc-aircraft-2013b\"\n",
        "images_base_dir = dataset_folder / \"data\" / \"images\" # Base directory where images are now in class subfolders\n",
        "\n",
        "# Define paths to the train and test mapping files\n",
        "train_mapping_file = dataset_folder / \"data\" / \"images_variant_train.txt\"\n",
        "test_mapping_file = dataset_folder / \"data\" / \"images_variant_test.txt\"\n",
        "variants_file = dataset_folder / \"data\" / \"variants.txt\" # File containing all class names\n",
        "\n",
        "# Define the target root directories for the train and test splits\n",
        "train_root_dir = dataset_folder / \"train\"\n",
        "test_root_dir = dataset_folder / \"test\"\n",
        "\n",
        "# Function to check if the directories are populated\n",
        "def is_populated(directory):\n",
        "    if not directory.exists():\n",
        "        return False\n",
        "    # Check if there are any subdirectories (which represent classes)\n",
        "    if not any(directory.iterdir()):\n",
        "        return False\n",
        "    # You could add a more robust check here, like checking if a certain number of\n",
        "    # class directories exist or if a specific file exists in a class directory.\n",
        "    return True\n",
        "\n",
        "# Check if directories are already populated\n",
        "if is_populated(train_root_dir) and is_populated(test_root_dir):\n",
        "    print(\"Train and test directories are already populated. Skipping population.\")\n",
        "else:\n",
        "    print(\"Train and test directories are not populated or incomplete. Populating now...\")\n",
        "\n",
        "    # Ensure the target root directories exist and are empty or can be overwritten if not populated\n",
        "    for dir_path in [train_root_dir, test_root_dir]:\n",
        "        if dir_path.exists() and not is_populated(dir_path):\n",
        "            print(f\"Removing incomplete existing directory: {dir_path}\")\n",
        "            shutil.rmtree(dir_path)\n",
        "        dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Get the full list of class names from variants.txt\n",
        "    all_class_names = []\n",
        "    with open(variants_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            all_class_names.append(line.strip())\n",
        "    all_class_names = sorted(list(set(all_class_names))) # Remove duplicates and sort\n",
        "\n",
        "    print(f\"Found {len(all_class_names)} unique class names from variants.txt.\")\n",
        "\n",
        "    # Create all class subdirectories within the train and test root directories\n",
        "    print(\"Creating all class subdirectories in train and test folders...\")\n",
        "    for class_name in all_class_names:\n",
        "        sanitized_class_folder_name = class_name.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
        "        (train_root_dir / sanitized_class_folder_name).mkdir(parents=True, exist_ok=True)\n",
        "        (test_root_dir / sanitized_class_folder_name).mkdir(parents=True, exist_ok=True)\n",
        "    print(\"Class subdirectories created.\")\n",
        "\n",
        "\n",
        "    # Function to read mapping files and get image ID to label mapping\n",
        "    def read_mapping_file(filepath):\n",
        "        image_id_to_class = {}\n",
        "        with open(filepath, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(\" \", 1)\n",
        "                if len(parts) == 2:\n",
        "                    image_id, label = parts\n",
        "                    image_id_to_class[image_id] = label\n",
        "                else:\n",
        "                     print(f\"Warning: Skipping line in {filepath.name} with unexpected format: {line.strip()}\")\n",
        "        return image_id_to_class\n",
        "\n",
        "    # Read mappings for train and test sets\n",
        "    train_image_id_to_class = read_mapping_file(train_mapping_file)\n",
        "    test_image_id_to_class = read_mapping_file(test_mapping_file)\n",
        "\n",
        "    print(f\"Found mappings for {len(train_image_id_to_class)} training images.\")\n",
        "    print(f\"Found mappings for {len(test_image_id_to_class)} testing images.\")\n",
        "\n",
        "    # Function to copy images for a given split based on mappings\n",
        "    def populate_split_directories(image_id_to_class_mapping, target_root_dir, images_base_dir):\n",
        "        copied_count = 0\n",
        "        source_not_found_count = 0\n",
        "        target_folder_not_found_count = 0\n",
        "\n",
        "        print(f\"Populating {target_root_dir} with images...\")\n",
        "\n",
        "        for image_id, class_label in image_id_to_class_mapping.items():\n",
        "            image_name = f\"{image_id}.jpg\"\n",
        "            sanitized_class_folder_name = class_label.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
        "\n",
        "            # Source path: Look for the image within the existing class subfolders in images_base_dir\n",
        "            source_image_path = images_base_dir / sanitized_class_folder_name / image_name\n",
        "\n",
        "            # Target path: The location in the new train/test class folder (already created)\n",
        "            target_class_folder = target_root_dir / sanitized_class_folder_name\n",
        "            target_image_path = target_class_folder / image_name\n",
        "\n",
        "            if source_image_path.exists():\n",
        "                if target_class_folder.exists(): # Ensure target class folder was created\n",
        "                    if not target_image_path.exists():\n",
        "                        try:\n",
        "                            shutil.copy(str(source_image_path), str(target_image_path))\n",
        "                            copied_count += 1\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error copying file {source_image_path} to {target_class_folder}: {e}\")\n",
        "                else:\n",
        "                    print(f\"Error: Target class folder not found for {class_label}: {target_class_folder}\")\n",
        "                    target_folder_not_found_count += 1\n",
        "            else:\n",
        "                print(f\"Warning: Source image file not found: {source_image_path}\")\n",
        "                source_not_found_count += 1\n",
        "\n",
        "        print(f\"Finished populating {target_root_dir}. Copied {copied_count} images. {source_not_found_count} source images not found. {target_folder_not_found_count} target class folders not found.\")\n",
        "\n",
        "\n",
        "    # Populate train and test directories\n",
        "    populate_split_directories(train_image_id_to_class, train_root_dir, images_base_dir)\n",
        "    populate_split_directories(test_image_id_to_class, test_root_dir, images_base_dir)\n",
        "\n",
        "\n",
        "    print(\"Dataset reorganization and population complete.\")"
      ],
      "metadata": {
        "id": "4Id2xd_0eSLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will have to put new cell here and make some edits to change how the data is loaded. Currently, I have the dataset downloaded to my drive and the dataloaders have to take all of the data from their piece by piece, but it should be much faster to instead upon each section, copy all data from drive to the local colab vm and then continue"
      ],
      "metadata": {
        "id": "nlxMipUrZqx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Copying dataset from Google Drive to local VM...\")\n",
        "print(\"This may take a few minutes, but it's a one-time operation.\")\n",
        "\n",
        "# Define local paths\n",
        "local_train_dir = \"/content/train\"\n",
        "local_test_dir = \"/content/test\"\n",
        "\n",
        "# Define GDrive paths\n",
        "gdrive_train_dir = \"/content/drive/MyDrive/data/fgvc-aircraft-2013b/train\"\n",
        "gdrive_test_dir = \"/content/drive/MyDrive/data/fgvc-aircraft-2013b/test\"\n",
        "\n",
        "# Run the copy commands (only if not already copied)\n",
        "if not os.path.exists(local_train_dir):\n",
        "    print(\"Copying train set...\")\n",
        "    !cp -r {gdrive_train_dir} {local_train_dir}\n",
        "    print(\"Train set copied.\")\n",
        "else:\n",
        "    print(\"Train set already on local VM.\")\n",
        "\n",
        "if not os.path.exists(local_test_dir):\n",
        "    print(\"Copying test set...\")\n",
        "    !cp -r {gdrive_test_dir} {local_test_dir}\n",
        "    print(\"Test set copied.\")\n",
        "else:\n",
        "    print(\"Test set already on local VM.\")\n",
        "\n",
        "print(\"Copying complete. You can now proceed.\")"
      ],
      "metadata": {
        "id": "n8mog7TYbKG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-12-e84a44c78b2d\n",
        "# Assume the dataset is extracted to 'data/fgvc-aircraft-2013b'\n",
        "# and images are in 'data/fgvc-aircraft-2013b/data/images'\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "#data_path = Path(\"/content/drive/MyDrive/data/\") # This variable is defined in the first cell\n",
        "#dataset_folder = data_path / \"fgvc-aircraft-2013b\"\n",
        "\n",
        "#train_dir = dataset_folder / \"data\" / \"images\"\n",
        "#test_dir = dataset_folder / \"data\" / \"images\"\n",
        "\n",
        "train_dir = \"/content/train\"\n",
        "test_dir = \"/content/test\"\n",
        "\n",
        "\n",
        "# Print the resolved paths to verify they are correct\n",
        "print(\"Train directory:\", train_dir)\n",
        "print(\"Test directory:\", test_dir)"
      ],
      "metadata": {
        "id": "tKnebh8NpT7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3483b4-2780-475f-a6f6-b3a47ae1a0b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train directory: /content/drive/MyDrive/data/fgvc-aircraft-2013b/data/images\n",
            "Test directory: /content/drive/MyDrive/data/fgvc-aircraft-2013b/data/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we continue on to creating our datasets and dataloaders. An important issue is that we have to ensure that the data that we feed into our pretrained model must be formatted in the same way as the data inputted when training the model (as that helps performance immeasurably). There is a certain way that all models from torchvision.models require, and we will do that.\n",
        "\n",
        "It is detailed in this page: https://docs.pytorch.org/vision/0.8/models.html"
      ],
      "metadata": {
        "id": "JWvJSUvVtJYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import importlib\n",
        "import setup_dataholders\n",
        "importlib.reload(setup_dataholders)\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each color channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each color channel),\n",
        "])"
      ],
      "metadata": {
        "id": "I2wsd6E2t64w"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = setup_dataholders.create_dataloaders(train_directory=train_dir,\n",
        "                                                                               test_directory=test_dir,\n",
        "                                                                               data_transforms=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n",
        "                                                                               batch_size=32, # set mini-batch size to 32\n",
        "                                                                               workers=0)\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "TC7PU_cxr7He",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd28869-bcf0-4168-81af-5ec497211a4e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7efd2b02d250>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7efd2b02c200>,\n",
              " ['707_320',\n",
              "  '727_200',\n",
              "  '737_200',\n",
              "  '737_300',\n",
              "  '737_400',\n",
              "  '737_500',\n",
              "  '737_600',\n",
              "  '737_700',\n",
              "  '737_800',\n",
              "  '737_900',\n",
              "  '747_100',\n",
              "  '747_200',\n",
              "  '747_300',\n",
              "  '747_400',\n",
              "  '757_200',\n",
              "  '757_300',\n",
              "  '767_200',\n",
              "  '767_300',\n",
              "  '767_400',\n",
              "  '777_200',\n",
              "  '777_300',\n",
              "  'A300B4',\n",
              "  'A310',\n",
              "  'A318',\n",
              "  'A319',\n",
              "  'A320',\n",
              "  'A321',\n",
              "  'A330_200',\n",
              "  'A330_300',\n",
              "  'A340_200',\n",
              "  'A340_300',\n",
              "  'A340_500',\n",
              "  'A340_600',\n",
              "  'A380',\n",
              "  'ATR_42',\n",
              "  'ATR_72',\n",
              "  'An_12',\n",
              "  'BAE_125',\n",
              "  'BAE_146_200',\n",
              "  'BAE_146_300',\n",
              "  'Beechcraft_1900',\n",
              "  'Boeing_717',\n",
              "  'CRJ_200',\n",
              "  'CRJ_700',\n",
              "  'CRJ_900',\n",
              "  'C_130',\n",
              "  'C_47',\n",
              "  'Cessna_172',\n",
              "  'Cessna_208',\n",
              "  'Cessna_525',\n",
              "  'Cessna_560',\n",
              "  'Challenger_600',\n",
              "  'DC_10',\n",
              "  'DC_3',\n",
              "  'DC_6',\n",
              "  'DC_8',\n",
              "  'DC_9_30',\n",
              "  'DHC_1',\n",
              "  'DHC_6',\n",
              "  'DHC_8_100',\n",
              "  'DHC_8_300',\n",
              "  'DH_82',\n",
              "  'DR_400',\n",
              "  'Dornier_328',\n",
              "  'EMB_120',\n",
              "  'ERJ_135',\n",
              "  'ERJ_145',\n",
              "  'E_170',\n",
              "  'E_190',\n",
              "  'E_195',\n",
              "  'Embraer_Legacy_600',\n",
              "  'Eurofighter_Typhoon',\n",
              "  'F_16A_B',\n",
              "  'F_A_18',\n",
              "  'Falcon_2000',\n",
              "  'Falcon_900',\n",
              "  'Fokker_100',\n",
              "  'Fokker_50',\n",
              "  'Fokker_70',\n",
              "  'Global_Express',\n",
              "  'Gulfstream_IV',\n",
              "  'Gulfstream_V',\n",
              "  'Hawk_T1',\n",
              "  'Il_76',\n",
              "  'L_1011',\n",
              "  'MD_11',\n",
              "  'MD_80',\n",
              "  'MD_87',\n",
              "  'MD_90',\n",
              "  'Metroliner',\n",
              "  'Model_B200',\n",
              "  'PA_28',\n",
              "  'SR_20',\n",
              "  'Saab_2000',\n",
              "  'Saab_340',\n",
              "  'Spitfire',\n",
              "  'Tornado',\n",
              "  'Tu_134',\n",
              "  'Tu_154',\n",
              "  'Yak_42'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cells focus on the actual \"transfer\" part of taking a model from someplace else and using it for better performance. I could have done the transforms from the previous cells in a different way that is more automatic, but I wished to explore the more manual original way first."
      ],
      "metadata": {
        "id": "xG8FesLvNe3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now onto taking a different model and applying it here! We will use the [EfficentNet_v2_m](https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_v2_m.html#torchvision.models.efficientnet_v2_m) model."
      ],
      "metadata": {
        "id": "FS12EZlOmSSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
        "\n",
        "# Take the precalculated features, avgpool calculation, and the classifier from the transfered model and apply it to our problem!"
      ],
      "metadata": {
        "id": "Lju21leDmfw7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a summary using torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "summary(model=model,\n",
        "        input_size=(32, 3, 480, 480), # make sure this is \"input_size\", not \"input_shape\"\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "id": "MEiusxPMpMBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b51cce-9128-4463-e09b-e061a3da5e92"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [32, 3, 480, 480]    [32, 1000]           --                   True\n",
              "├─Sequential (features)                                      [32, 3, 480, 480]    [32, 1280, 15, 15]   --                   True\n",
              "│    └─Conv2dNormActivation (0)                              [32, 3, 480, 480]    [32, 32, 240, 240]   --                   True\n",
              "│    │    └─Conv2d (0)                                       [32, 3, 480, 480]    [32, 32, 240, 240]   864                  True\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 32, 240, 240]   [32, 32, 240, 240]   64                   True\n",
              "│    │    └─SiLU (2)                                         [32, 32, 240, 240]   [32, 32, 240, 240]   --                   --\n",
              "│    └─Sequential (1)                                        [32, 32, 240, 240]   [32, 16, 240, 240]   --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 32, 240, 240]   [32, 16, 240, 240]   1,448                True\n",
              "│    └─Sequential (2)                                        [32, 16, 240, 240]   [32, 24, 120, 120]   --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 16, 240, 240]   [32, 24, 120, 120]   6,004                True\n",
              "│    │    └─MBConv (1)                                       [32, 24, 120, 120]   [32, 24, 120, 120]   10,710               True\n",
              "│    └─Sequential (3)                                        [32, 24, 120, 120]   [32, 40, 60, 60]     --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 24, 120, 120]   [32, 40, 60, 60]     15,350               True\n",
              "│    │    └─MBConv (1)                                       [32, 40, 60, 60]     [32, 40, 60, 60]     31,290               True\n",
              "│    └─Sequential (4)                                        [32, 40, 60, 60]     [32, 80, 30, 30]     --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 40, 60, 60]     [32, 80, 30, 30]     37,130               True\n",
              "│    │    └─MBConv (1)                                       [32, 80, 30, 30]     [32, 80, 30, 30]     102,900              True\n",
              "│    │    └─MBConv (2)                                       [32, 80, 30, 30]     [32, 80, 30, 30]     102,900              True\n",
              "│    └─Sequential (5)                                        [32, 80, 30, 30]     [32, 112, 30, 30]    --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 80, 30, 30]     [32, 112, 30, 30]    126,004              True\n",
              "│    │    └─MBConv (1)                                       [32, 112, 30, 30]    [32, 112, 30, 30]    208,572              True\n",
              "│    │    └─MBConv (2)                                       [32, 112, 30, 30]    [32, 112, 30, 30]    208,572              True\n",
              "│    └─Sequential (6)                                        [32, 112, 30, 30]    [32, 192, 15, 15]    --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 112, 30, 30]    [32, 192, 15, 15]    262,492              True\n",
              "│    │    └─MBConv (1)                                       [32, 192, 15, 15]    [32, 192, 15, 15]    587,952              True\n",
              "│    │    └─MBConv (2)                                       [32, 192, 15, 15]    [32, 192, 15, 15]    587,952              True\n",
              "│    │    └─MBConv (3)                                       [32, 192, 15, 15]    [32, 192, 15, 15]    587,952              True\n",
              "│    └─Sequential (7)                                        [32, 192, 15, 15]    [32, 320, 15, 15]    --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 192, 15, 15]    [32, 320, 15, 15]    717,232              True\n",
              "│    └─Conv2dNormActivation (8)                              [32, 320, 15, 15]    [32, 1280, 15, 15]   --                   True\n",
              "│    │    └─Conv2d (0)                                       [32, 320, 15, 15]    [32, 1280, 15, 15]   409,600              True\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 1280, 15, 15]   [32, 1280, 15, 15]   2,560                True\n",
              "│    │    └─SiLU (2)                                         [32, 1280, 15, 15]   [32, 1280, 15, 15]   --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 15, 15]   [32, 1280, 1, 1]     --                   --\n",
              "├─Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   True\n",
              "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
              "│    └─Linear (1)                                            [32, 1280]           [32, 1000]           1,281,000            True\n",
              "============================================================================================================================================\n",
              "Total params: 5,288,548\n",
              "Trainable params: 5,288,548\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 56.47\n",
              "============================================================================================================================================\n",
              "Input size (MB): 88.47\n",
              "Forward/backward pass size (MB): 15843.11\n",
              "Params size (MB): 21.15\n",
              "Estimated Total Size (MB): 15952.74\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to freeze some layers of this model because that means we do not have to train them again!"
      ],
      "metadata": {
        "id": "ugnPgmwo-eNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "Myay1QAK-oE8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update our model to now use 102 classes instead, since our dataset only had 102 variants"
      ],
      "metadata": {
        "id": "TnfKhpDK-9zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# From before\n",
        "output_shape = len(class_names)\n",
        "# Recreate the classifier layer and seed it to the target device\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(in_features=1280,\n",
        "                    out_features=output_shape, # same number of output units as our number of classes\n",
        "                    bias=True)).to(device)"
      ],
      "metadata": {
        "id": "FG0xSRho-ypn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Classifier device: {model.classifier[1].weight.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBX2l29iFx16",
        "outputId": "426979e8-6b1b-43f8-f15f-a7d996c61b81"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model device: cuda:0\n",
            "Classifier device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model,\n",
        "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
        "        verbose=0,\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaL5GEqDA_FH",
        "outputId": "e6bf7899-ae1e-498f-e48e-de1422f990e4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 100]            --                   Partial\n",
              "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
              "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
              "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
              "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
              "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
              "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
              "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
              "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
              "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
              "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
              "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
              "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
              "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
              "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
              "├─Sequential (classifier)                                    [32, 1280]           [32, 100]            --                   True\n",
              "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
              "│    └─Linear (1)                                            [32, 1280]           [32, 100]            128,100              True\n",
              "============================================================================================================================================\n",
              "Total params: 4,135,648\n",
              "Trainable params: 128,100\n",
              "Non-trainable params: 4,007,548\n",
              "Total mult-adds (Units.GIGABYTES): 12.31\n",
              "============================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3452.12\n",
              "Params size (MB): 16.54\n",
              "Estimated Total Size (MB): 3487.93\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "AVLqWLGTBsDp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Check setup first\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING SETUP CHECK\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Training samples: {len(train_dataloader.dataset)}\")\n",
        "print(f\"Training batches: {len(train_dataloader)}\")\n",
        "print(f\"Testing samples: {len(test_dataloader.dataset)}\")\n",
        "print(f\"Testing batches: {len(test_dataloader)}\")\n",
        "print(f\"Batch size: {train_dataloader.batch_size}\")\n",
        "print(f\"Workers: {train_dataloader.num_workers}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test one batch first\n",
        "print(\"\\nTesting forward pass with one batch...\")\n",
        "images, labels = next(iter(train_dataloader))\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(images)\n",
        "    print(f\"✓ Forward pass successful! Output shape: {output.shape}\")\n",
        "model.train()\n",
        "\n",
        "# Now train\n",
        "print(\"\\nStarting training... (this may take 5-10 minutes per epoch)\")\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "results = model_backbone.train(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    epochs=5,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708,
          "referenced_widgets": [
            "6ac90e26fa9f47009549e22f73170029",
            "70ec9ec658ec4125964f02351ffd5976",
            "29de0981d2f341b581573b7250035a57",
            "414d7c2038e3468b8a398e40b4e8aeac",
            "a1b8f9d921044ef1aef754021121141e",
            "0b05200401df4109b3606d689f5ae176",
            "f907664db6a74661b4d84ef87c226693",
            "711cfba8dbe14c1da398704d41387765",
            "14bd0499e0774801ab0905ce651e13e1",
            "c167c71658184efb9943b0a3d40880d9",
            "b252ab4183e44b6db7ca8e5c70c1bd8e"
          ]
        },
        "id": "Kxv-ruwaBz1C",
        "outputId": "4d382510-dbc0-41f6-95f5-e4854b08db62"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TRAINING SETUP CHECK\n",
            "============================================================\n",
            "Device: cuda\n",
            "Model device: cuda:0\n",
            "Training samples: 3334\n",
            "Training batches: 105\n",
            "Testing samples: 3333\n",
            "Testing batches: 105\n",
            "Batch size: 32\n",
            "Workers: 0\n",
            "============================================================\n",
            "\n",
            "Testing forward pass with one batch...\n",
            "✓ Forward pass successful! Output shape: torch.Size([32, 100])\n",
            "\n",
            "Starting training... (this may take 5-10 minutes per epoch)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ac90e26fa9f47009549e22f73170029"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-542863308.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m results = model_backbone.train(\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/PyTorchMLEngine-Custom-Dataset-Project/model_backbone.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# Perform a single training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         train_loss, train_acc = train_step(\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/PyTorchMLEngine-Custom-Dataset-Project/model_backbone.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Iterate over batches in the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Move data to the specified device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3522\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3524\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3526\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OHSTwf4nC_CY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}